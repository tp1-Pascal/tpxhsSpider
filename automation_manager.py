import asyncio
import random
from pathlib import Path
from datetime import datetime
from browser_manager import BrowserManager
from scraper import extract_note_data, save_to_json, save_pending_urls, load_pending_urls
from process_result import process_keyword_results

# é…ç½®
KEYWORDS_FILE = Path("keywords.md")
MAX_NOTES_PER_KEYWORD = 5 # Fallback default

def parse_keywords(file_path: Path):
    """è§£æå…³é”®è¯æ–‡ä»¶ï¼Œæ”¯æŒ "å…³é”®è¯" æˆ– "å…³é”®è¯: æ•°é‡" æ ¼å¼"""
    if not file_path.exists():
        return []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    keywords_data = []
    for line in lines:
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        
        # å¤„ç† Markdown åˆ—è¡¨æ ¼å¼ "- å…³é”®è¯"
        if line.startswith('- '):
            content = line[2:].strip()
        else:
            content = line
            
        # è§£ææ•°é‡ (ä¾‹å¦‚ "å¤±çœ : 10")
        parts = content.split(':', 1)
        keyword = parts[0].strip()
        count = MAX_NOTES_PER_KEYWORD
        
        if len(parts) > 1:
            try:
                count = int(parts[1].strip())
            except ValueError:
                pass
                
        keywords_data.append({"keyword": keyword, "count": count})
            
    return keywords_data

async def run_automation():
    """ä¸»æ‰§è¡Œé€»è¾‘ - Standalone Version"""
    print("ğŸš€ å¯åŠ¨å°çº¢ä¹¦è‡ªåŠ¨æŠ“å– (Standalone Mode)")
    
    # 1. å‡†å¤‡å…³é”®è¯
    all_keywords_data = parse_keywords(KEYWORDS_FILE)
    if not all_keywords_data:
        print("âŒ æœªèƒ½è§£æåˆ°å…³é”®è¯ï¼Œè¯·æ£€æŸ¥ keywords.md")
        return

    print(f"ğŸ“‹ å…± {len(all_keywords_data)} ä¸ªå…³é”®è¯å¾…å¤„ç†ã€‚")
    
    # 2. åˆå§‹åŒ–æµè§ˆå™¨
    # å»ºè®®é¦–æ¬¡è¿è¡Œä½¿ç”¨ headless=False ä»¥ä¾¿è§‚å¯Ÿå’Œæ‰‹åŠ¨è¿‡éªŒè¯
    browser = BrowserManager(headless=False) 
    await browser.start()
    
    # 3. ç™»å½•æ£€æŸ¥ä¸äº¤äº’
    print("â³æ­£åœ¨æ£€æŸ¥ç™»å½•çŠ¶æ€...")
    # Go to home page to check login status
    await browser.page.goto("https://www.xiaohongshu.com")
    is_logged_in = await browser.check_login_status()
    
    if not is_logged_in:
        print("\nâš ï¸  æ£€æµ‹åˆ°æœªç™»å½•ï¼")
        print("è¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨çª—å£ä¸­æ‰‹åŠ¨ç™»å½•å°çº¢ä¹¦è´¦æˆ·ã€‚")
        print("ç™»å½•å®Œæˆåï¼Œè¯·åœ¨ä¸‹æ–¹è¾“å…¥ 'yes' ç»§ç»­...")
        
        while True:
            # Note: In an async loop, input() blocks. For a simple script this is fine.
            user_input = input(">> æ˜¯å¦å·²å®Œæˆç™»å½•ï¼Ÿ(yes/no): ").strip().lower()
            if user_input == 'yes':
                print("â³ æ­£åœ¨åˆ·æ–°é¡µé¢ä»¥ç¡®è®¤çŠ¶æ€...")
                await browser.page.reload(wait_until="domcontentloaded")
                await asyncio.sleep(2) # Wait for hydration
                
                print("â³ æ­£åœ¨å†æ¬¡æ ¡éªŒç™»å½•çŠ¶æ€...")
                is_logged_in = await browser.check_login_status()
                if is_logged_in:
                    print("âœ… æ ¡éªŒæˆåŠŸï¼ç»§ç»­æ‰§è¡Œ...")
                    break
                else:
                    print("âŒ æ£€æµ‹åˆ°ä»æœªç™»å½•ï¼Œè¯·ç¡®ä¿é¡µé¢æ˜¾ç¤ºå·²ç™»å½•çŠ¶æ€ï¼ˆå¦‚çœ‹åˆ°å¤´åƒï¼‰ã€‚")
            elif user_input == 'no':
                print("é€€å‡ºç¨‹åºã€‚")
                await browser.close()
                return
    else:
        print("âœ… æ£€æµ‹åˆ°å·²ç™»å½•ï¼Œè‡ªåŠ¨ç»§ç»­...")
    
    try:
        for item in all_keywords_data:
            keyword = item['keyword']
            target_count = item['count']
            
            print(f"\nğŸ” æ­£åœ¨å¤„ç†å…³é”®è¯: {keyword} (ç›®æ ‡æ•°é‡: {target_count})")
            
            try:
                # 3.1 è¿›å…¥æœç´¢é¡µ
                await browser.goto_search_page(keyword)
                
                # 3.2 è·å–ç¬”è®°é“¾æ¥
                urls = await browser.get_search_results(count=target_count)
                print(f"ğŸ”— æ‰¾åˆ° {len(urls)} ä¸ªç¬”è®°é“¾æ¥")
                
                new_items = []
                for url in urls:
                    try:
                        # 3.3 æŠ“å–å†…å®¹
                        # Go to detail page directly
                        data = await browser.extract_note_content(url)
                        if data:
                            data['url'] = url
                            new_items.append(data)
                            print(f"   âœ… Saved: {data.get('title', 'No Title')[:20]}...")
                        else:
                             print(f"   âš ï¸ Failed to extract content from {url}")
                             
                    except Exception as e:
                        print(f"âŒ Error scraping {url}: {e}")
                        continue
                
                if new_items:
                    print(f"ğŸ“¥ Processing images for {len(new_items)} items...")
                    process_keyword_results(keyword, new_items, total_keywords=len(all_keywords_data))
                
                wait_time = random.uniform(5, 10)
                print(f"ğŸ’¤ å…³é”®è¯é—´ä¼‘æ¯ {wait_time:.1f} ç§’...")
                await asyncio.sleep(wait_time)
                
            except Exception as e:
                 print(f"âŒ Error processing keyword '{keyword}': {e}")
                 # Try to recover navigation
                 try:
                     await browser.page.goto("https://www.xiaohongshu.com")
                 except: pass
                 continue
                
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ç»ˆæ­¢äº†ç¨‹åºã€‚")
    finally:
        await browser.close()
        print("ğŸ ä»»åŠ¡å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(run_automation())
